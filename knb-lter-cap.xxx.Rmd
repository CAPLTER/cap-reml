---
title: "remlTemplate"
author: "SRE"
date: Sys.Date()
output: html_document
editor_options: 
  chunk_output_type: console
params:
  projectScope: "LTER"
---

```{r libraries}
library(EML)
library(tidyverse)
library(readxl)
library(capeml)
library(gioseml)
```

```{r dataset_details}
projectid <- 
packageIdent <- 'knb-lter-cap.xxx.x'
```

```{r connections::postgres::local, eval=FALSE}
# functions and setting for connecting to postgres
source('~/Documents/localSettings/pg_local.R')
pg <- pg_local
```

```{r connections::postgres::prod, eval=T }
# functions and setting for connecting to postgres
source('~/Documents/localSettings/pg_prod.R')
pg <- pg_prod
```

```{r connections::mysql::prod, eval=T }
# functions and setting for connecting to mysql
source('~/Documents/localSettings/mysql_prod.R')
mysql_prod <- mysql_prod_connect()
```

```{r compiled_data, eval=TRUE}

# tabularDataEntity <- import / generate...process...

write_attributes(tabularDataEntity)
write_factors(tabularDataEntity)

tabularDataEntity_desc <- "desc"

tabularDataEntity_DT <- create_dataTable(dfname = tabularDataEntity,
                                         description = tabularDataEntity_desc)

```

```{r otherEntity, eval=TRUE}

otherEntityObject_OE <- create_otherEntity(targetFile = 'kml_pdf_etc',
                                           description = 'desc')

```

```{r spatialVector, eval=TRUE}

library(sf) # additional library

# note layer name is sans file extension
spatialVectorEntity <- read_sf(dsn = "directory/", layer = "layer name")

# assign Name to the column that is the site identifier
spatialVectorEntity <- spatialVectorEntity %>% 
  mutate(Name = site identifier)

write_attributes(spatialVectorEntity)

spatialVectorEntity_desc <- "desc"

spatialVector_SV <- create_spatialVector(targetFile = spatialVectorEntity,
                                         description = spatialVectorEntityDesc )

```

```{r title}

title <- ''
```

See the gioseml package for examples of creating people resources from scratch.

```{r people}

# creator(s) - required

cameron <- create_role(firstName = "cameron",
                       lastName = "boehme",
                       roleType = "creator")
chelsea <- create_role(firstName = "chelsea",
                       lastName = "stratton",
                       roleType = "creator")
fabio <- create_role(giosPersonId = 23857,
                     roleType = "creator")

creators <- list(cameron, chelsea, fabio)

# metadata provider - required

cameron <- create_role(firstName = "cameron",
                       lastName = "boehme",
                       roleType = "metadata")

# associated party - optional
# takes the optional argument `projectRole` (default: "former project associate")
mark <- create_role(giosPersonId = 132,
                    roleType = "associated")
nancy <- create_role(giosPersonId = 150,
                     roleType = "associated")

associatedParty <- list(mark, nancy)


```

```{r keywords}

# CAP IRTs for reference (be sure to include these as appropriate):
# https://sustainability.asu.edu/caplter/research/

write_keywords()
```

Use this extended approach of developing methods if provenance data are required
or there are multiple methods files, otherwise the `create_dateset()` function
will look for a methods.md file in the working directory (or a file path and
name can be passed).

```{r methods}

library(EDIutils)

# methods from file tagged as markdown
main <- read_markdown("methods.md")

# provenance: naip
naip <- emld::as_emld(EDIutils::api_get_provenance_metadata("knb-lter-cap.623.1"))
naip$`@context` <- NULL
naip$`@type` <- NULL

# provenance: lst
lst <- emld::as_emld(EDIutils::api_get_provenance_metadata("knb-lter-cap.677.1"))
lst$`@context` <- NULL
lst$`@type` <- NULL

enhancedMethods <- EML::eml$methods(methodStep = list(main, naip, lst))

```

```{r coverages}

# begindate <- format(min(runoff_chemistry$runoff_datetime), "%Y-%m-%d")
# enddate <- format(max(runoff_chemistry$runoff_datetime), "%Y-%m-%d")
geographicDescription <- "CAP LTER study area"
coverage <- set_coverage(begin = "2014-09-01",
                         end = "2015-03-30",
                         geographicDescription = geographicDescription,
                         west = -112.100, east = -111.877,
                         north = +33.608, south = +33.328)

```

Taxonomic coverage(s) are constructed using EDI's taxonomyCleanr tool suite.

*Note* that the `taxa_map.csv` built with the `create_taxa_map()` function and
resolving taxonomic IDs (i.e., `resolve_comm_taxa()`) only needs to be run once,
a potentially long process, per version/session -- the taxonomicCoverage can be
built as many times as needed with `resolve_comm_taxa()` once the `taxa_map.csv`
has been generated and the taxonomic IDs resolved.

```{r taxonomyCleanr, eval=TRUE}

library(taxonomyCleanr)

my_path <- getwd() # taxonomyCleanr requires a path (to build the taxa_map)

# Example: draw taxonomic information from existing resource:

# plant taxa listed in the om_transpiration_factors file
plantTaxa <- read_csv('om_transpiration_factors.csv') %>% 
  filter(attributeName == "species") %>% 
  as.data.frame()

# create or update map. A taxa_map.csv is the heart of taxonomyCleanr. This
# function will build the taxa_map.csv and put it in the path identified with
# my_path.
create_taxa_map(path = my_path, x = plantTaxa, col = "definition") 

# Example: construct taxonomic resource:

gambelQuail <- tibble(taxName = "Callipepla gambelii")

# Create or update map: a taxa_map.csv is the heart of taxonomyCleanr. This
# function will build the taxa_map.csv in the path identified with my_path.
create_taxa_map(path = my_path, x = gambelQuail, col = "taxName") 

# Resolve taxa by attempting to match the taxon name (data.source 3 is ITIS but
# other sources are accessible). Use `resolve_comm_taxa` instead of
# `resolve_sci_taxa` if taxa names are common names but note that ITIS
# (data.source 3) is the only authority taxonomyCleanr will allow for common
# names.
resolve_sci_taxa(path = my_path, data.sources = 3) # in this case, 3 is ITIS

# build the EML taxonomomic coverage
taxaCoverage <- make_taxonomicCoverage(path = my_path)

# add taxonomic to the other coverages
coverage$taxonomicCoverage <- taxaCoverage
```

```{r construct-dataset}

# optionally, provide: scope, abstract, methods, keywords, publication date
dataset <- create_dataset()
```

```{r dataSet$dataTable}

# add dataTables if relevant

print(ls(pattern = "_DT"))

if (length(ls(pattern = "_DT")) > 0) {
  
  listOfDataTables <- lapply(ls(pattern = "_DT"), function(DT) { get(DT) } )
  
  dataset$dataTable  <- listOfDataTables  
  
}

# or add manually
# dataset$dataTable <- list(dataTableOne, dataTableTwo)

```

```{r dataSet$otherEntity}

# add other entities if relevant

print(ls(pattern = "_OE"))

if (length(ls(pattern = "_OE")) > 0) {
  
  listOfOtherEntities <- lapply(ls(pattern = "_OE"), function(OE) { get(OE) } )
  
  dataset$otherEntity <- listOfOtherEntities 
  
}

# or add manually
# dataset$otherEntity <- list(otherEntityOne, otherEntityTwo)

```

```{r dataSet$spatialVector}

# add spatial vectors if relevant

print(ls(pattern = "_SV"))

if (length(ls(pattern = "_SV")) > 0) {
  
  listOfSpatialVectors <- lapply(ls(pattern = "_SV"), function(SV) { get(SV) } )
  
  dataset$spatialVector  <- listOfSpatialVectors  
  
}

# or add manually
# dataset$spatialVector <- list(spatialVectorOne, spatialVectorTwo)

```

```{r custom-units, eval=FALSE}

custom_units <- rbind(
  data.frame(id = "milligramPerKilogram",
             unitType = "massPerMass",
             parentSI = "gramsPerGram",
             multiplierToSI = 0.000001,
             description = "millgram of element per kilogram of material"))

unitList <- set_unitList(custom_units,
                         as_metadata = TRUE)

```

```{r literature cited, eval=TRUE}

# add literature cited if relevant
library(rcrossref)
library(EML)

mccafferty <- cr_cn(dois = "https://doi.org/10.1186/s40317-015-0075-2", format = "bibtex")
mccaffertycit <- EML::eml$citation(id = "https://doi.org/10.1186/s40317-015-0075-2")
mccaffertycit$bibtex <- mccafferty 

citations <- list(
  citation = list(
    mccaffertycit
  ) # close list of citations
) # close citation

dataset$literatureCited <- citations 

```

```{r construct_eml, eval=TRUE}

eml <- create_eml()
```

```{r write_eml}

# write the eml to file
write_eml(eml, paste0(packageIdent, ".xml"))
```

```{r preview_data_file_to_upload}

# preview data set files that will be uploaded to S3
list.files(pattern = paste0(projectid, "_"))
```

Move data and final xml files to respective ASU locations.

```{r S3_helper_functions}
# functions and setting for uploading to S3
library(aws.s3)
source('~/Documents/localSettings/aws.s3')
source('~/localRepos/reml-helper-tools/amazon_file_upload.R')
```

```{r upload_data_S3}

# upload files to S3
lapply(list.files(pattern = paste0(projectid, "_")), dataToAmz)
```

```{r clean_up}

# remove data files
dataFilesToRemove <- dir(pattern = paste0(projectid, "_"))
file.remove(dataFilesToRemove)

# EML to S3
if(length(list.files(pattern = "*.xml")) == 1) {
  emlToAmz(list.files(pattern = "*.xml")) } else {
    print("more than one xml file found")
  }

# EML to cap-data-eml and remove file from project
tryCatch({
  
  if(length(list.files(pattern = "*.xml")) == 1) {
    file.copy(list.files(pattern = "*.xml"), "/home/srearl/localRepos/cap-metadata/cap-data-eml/")
    file.remove(list.files(pattern = "*.xml")) } else {
      print("more than one xml file found")
    }
},
warning = function(warn) {
  print(paste("WARNING: ", warn))
},
error = function(err) {
  print(paste("ERROR: ", err))
  
}) # close try catch
```
